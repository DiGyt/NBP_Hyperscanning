{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening raw data file /net/store/nbp/projects/hyperscanning/hyperscanning-2.0/mne_data/sourcedata/sub-204/eeg/sub-204_task-hyper_eeg.fif...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-6d79c5da5c0a>:40: RuntimeWarning: This filename (/net/store/nbp/projects/hyperscanning/hyperscanning-2.0/mne_data/sourcedata/sub-204/eeg/sub-204_task-hyper_eeg.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz or _meg.fif\n",
      "  combined_raw = mne.io.read_raw_fif(subs_path, preload=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Isotrak not found\n",
      "    Range : 0 ... 3537048 =      0.000 ...  3454.148 secs\n",
      "Ready.\n",
      "Reading 0 ... 3537048  =      0.000 ...  3454.148 secs...\n",
      "EEG channel type selected for re-referencing\n",
      "Applying a custom EEG reference.\n",
      "EEG channel type selected for re-referencing\n",
      "Applying a custom EEG reference.\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.1 - 1.2e+02 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.10\n",
      "- Lower transition bandwidth: 0.10 Hz (-6 dB cutoff frequency: 0.05 Hz)\n",
      "- Upper passband edge: 120.00 Hz\n",
      "- Upper transition bandwidth: 30.00 Hz (-6 dB cutoff frequency: 135.00 Hz)\n",
      "- Filter length: 33793 samples (33.001 sec)\n",
      "\n",
      "6638 events found\n",
      "Event IDs: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 36 37 38 39 40 41 42 43 44 45 46 48 49]\n",
      "900 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "0 projection items activated\n",
      "Loading data for 900 events and 1537 original time points ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-6d79c5da5c0a>:90: RuntimeWarning: The events passed to the Epochs constructor are not chronologically ordered.\n",
      "  epochs = mne.Epochs(raw_combined, combined_events, tmin=tmin, tmax=tmax,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 bad epochs dropped\n",
      "Dropped 522 epochs: 0, 1, 2, 3, 4, 5, 7, 9, 10, 11, 12, 15, 16, 17, 18, 19, 20, 22, 23, 24, 27, 28, 29, 30, 31, 32, 34, 35, 36, 37, 38, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 56, 57, 59, 62, 63, 64, 65, 66, 67, 68, 69, 70, 72, 75, 76, 77, 78, 84, 85, 86, 90, 92, 93, 94, 96, 97, 98, 99, 100, 101, 102, 103, 107, 108, 109, 110, 111, 113, 114, 115, 117, 118, 121, 122, 123, 125, 126, 127, 128, 130, 131, 132, 133, 134, 135, 136, 138, 139, 142, 145, 150, 151, 152, 153, 155, 156, 157, 160, 167, 168, 170, 171, 173, 175, 177, 178, 179, 180, 181, 183, 186, 190, 194, 198, 203, 204, 205, 208, 213, 216, 217, 219, 221, 222, 223, 224, 225, 226, 227, 228, 229, 233, 235, 237, 239, 241, 244, 245, 246, 248, 250, 251, 253, 254, 255, 257, 260, 267, 269, 273, 276, 277, 279, 280, 281, 284, 290, 296, 299, 300, 301, 302, 303, 304, 305, 307, 309, 310, 311, 312, 315, 316, 317, 318, 319, 320, 322, 323, 324, 327, 328, 329, 330, 331, 332, 334, 335, 336, 337, 338, 340, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 356, 357, 359, 362, 363, 364, 365, 366, 367, 368, 369, 370, 372, 375, 376, 377, 378, 384, 385, 386, 390, 392, 393, 394, 396, 397, 398, 399, 400, 401, 402, 403, 407, 408, 409, 410, 411, 413, 414, 415, 417, 418, 421, 422, 423, 425, 426, 427, 428, 430, 431, 432, 433, 434, 435, 436, 438, 439, 442, 445, 450, 451, 452, 453, 455, 456, 457, 460, 467, 468, 470, 471, 473, 475, 477, 478, 479, 480, 481, 483, 486, 490, 494, 498, 503, 504, 505, 508, 513, 516, 517, 519, 521, 522, 523, 524, 525, 526, 527, 528, 529, 533, 535, 537, 539, 541, 544, 545, 546, 548, 550, 551, 553, 554, 555, 557, 560, 567, 569, 573, 576, 577, 579, 580, 581, 584, 590, 596, 599, 600, 601, 602, 603, 604, 605, 607, 609, 610, 611, 612, 615, 616, 617, 618, 619, 620, 622, 623, 624, 627, 628, 629, 630, 631, 632, 634, 635, 636, 637, 638, 640, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 656, 657, 659, 662, 663, 664, 665, 666, 667, 668, 669, 670, 672, 675, 676, 677, 678, 684, 685, 686, 690, 692, 693, 694, 696, 697, 698, 699, 700, 701, 702, 703, 707, 708, 709, 710, 711, 713, 714, 715, 717, 718, 721, 722, 723, 725, 726, 727, 728, 730, 731, 732, 733, 734, 735, 736, 738, 739, 742, 745, 750, 751, 752, 753, 755, 756, 757, 760, 767, 768, 770, 771, 773, 775, 777, 778, 779, 780, 781, 783, 786, 790, 794, 798, 803, 804, 805, 808, 813, 816, 817, 819, 821, 822, 823, 824, 825, 826, 827, 828, 829, 833, 835, 837, 839, 841, 844, 845, 846, 848, 850, 851, 853, 854, 855, 857, 860, 867, 869, 873, 876, 877, 879, 880, 881, 884, 890, 896, 899\n",
      "Reading /net/store/nbp/projects/hyperscanning/study_project/marked_data/bad_components/sub-204_p-0-ica.fif ...\n",
      "Now restoring ICA solution ...\n",
      "Ready.\n",
      "Transforming to ICA space (63 components)\n",
      "Zeroing out 19 ICA components\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a93201262c1a48cfada50dc139c3f5ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Repairing epochs', layout=Layout(flex='2'), max=126.0, st…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading /net/store/nbp/projects/hyperscanning/study_project/marked_data/bad_components/sub-204_p-0-ica.fif ...\n",
      "Now restoring ICA solution ...\n",
      "Ready.\n",
      "Transforming to ICA space (63 components)\n",
      "Zeroing out 19 ICA components\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6768661f3585423681493452285563e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Repairing epochs', layout=Layout(flex='2'), max=126.0, st…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading /net/store/nbp/projects/hyperscanning/study_project/marked_data/bad_components/sub-204_p-0-ica.fif ...\n",
      "Now restoring ICA solution ...\n",
      "Ready.\n",
      "Transforming to ICA space (63 components)\n",
      "Zeroing out 19 ICA components\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "240f6e36a56b48c4b1da8705f85e8055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Repairing epochs', layout=Layout(flex='2'), max=126.0, st…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "378 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "0 bad epochs dropped\n",
      "EEG channel type selected for re-referencing\n",
      "Applying average reference.\n",
      "Applying a custom EEG reference.\n",
      "Fitted sphere radius:         95.1 mm\n",
      "Origin head coordinates:      0.9 6.5 47.0 mm\n",
      "Origin device coordinates:    0.9 6.5 47.0 mm\n",
      "Reading /net/store/nbp/projects/hyperscanning/study_project/marked_data/bad_components/sub-204_p-1-ica.fif ...\n",
      "Now restoring ICA solution ...\n",
      "Ready.\n",
      "Transforming to ICA space (63 components)\n",
      "Zeroing out 12 ICA components\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78a4d8fd604f4697a256e468567fb34b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Repairing epochs', layout=Layout(flex='2'), max=126.0, st…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading /net/store/nbp/projects/hyperscanning/study_project/marked_data/bad_components/sub-204_p-1-ica.fif ...\n",
      "Now restoring ICA solution ...\n",
      "Ready.\n",
      "Transforming to ICA space (63 components)\n",
      "Zeroing out 12 ICA components\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7019dcbb2ad0436486368146c1220ad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Repairing epochs', layout=Layout(flex='2'), max=126.0, st…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading /net/store/nbp/projects/hyperscanning/study_project/marked_data/bad_components/sub-204_p-1-ica.fif ...\n",
      "Now restoring ICA solution ...\n",
      "Ready.\n",
      "Transforming to ICA space (63 components)\n",
      "Zeroing out 12 ICA components\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8debcf95bc93407882d9d36baea62f34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Repairing epochs', layout=Layout(flex='2'), max=126.0, st…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "378 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "0 bad epochs dropped\n",
      "EEG channel type selected for re-referencing\n",
      "Applying average reference.\n",
      "Applying a custom EEG reference.\n",
      "Fitted sphere radius:         95.1 mm\n",
      "Origin head coordinates:      0.9 6.5 47.0 mm\n",
      "Origin device coordinates:    0.9 6.5 47.0 mm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=7)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  11 tasks      | elapsed:   13.0s\n",
      "[Parallel(n_jobs=7)]: Done  65 tasks      | elapsed:   40.7s\n",
      "[Parallel(n_jobs=7)]: Done 128 out of 128 | elapsed:  1.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=7)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  11 tasks      | elapsed:    7.5s\n",
      "[Parallel(n_jobs=7)]: Done  65 tasks      | elapsed:   35.2s\n",
      "[Parallel(n_jobs=7)]: Done 128 out of 128 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=7)]: Using backend LokyBackend with 7 concurrent workers.\n",
      "[Parallel(n_jobs=7)]: Done  11 tasks      | elapsed:    7.6s\n",
      "[Parallel(n_jobs=7)]: Done  65 tasks      | elapsed:   35.2s\n",
      "[Parallel(n_jobs=7)]: Done 128 out of 128 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path as op\n",
    "module_path = op.abspath(op.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mne\n",
    "import networkx as nx\n",
    "import h5py\n",
    "from scipy.io import savemat, loadmat\n",
    "from autoreject.autoreject import _apply_interp\n",
    "\n",
    "\n",
    "from data_analysis.functions_preprocessing import \\\n",
    "    (combine_raws, split_raws, combine_epochs, split_epochs,\n",
    "     preprocess_single_sub, load_ica, load_autoreject)\n",
    "from data_analysis.functions_behavioral import \\\n",
    "    (create_event_df, remove_ghost_triggers, calculate_alpha,\n",
    "     join_event_dfs, remove_outliers, events_from_event_df)\n",
    "\n",
    "# number of cores to use for parallel processing (ramsauer pc should have 80 cores)\n",
    "n_jobs = 7\n",
    "\n",
    "subject_dir = \"/net/store/nbp/projects/hyperscanning/hyperscanning-2.0/mne_data/sourcedata/\"\n",
    "behav_dir = \"/net/store/nbp/projects/hyperscanning/study_project/NBP_Hyperscanning/data_analysis/Behavioural_Analysis/BehaviouralData\"\n",
    "result_dir = \"/net/store/nbp/projects/hyperscanning/study_project/results\"\n",
    "\n",
    "\n",
    "# Main Data Analysis ###################################\n",
    "\n",
    "\n",
    "# Perform the data analysis\n",
    "for subj_pair in ['204']:\n",
    "\n",
    "    subs_path = subject_dir + \"sub-{0}/eeg/sub-{0}_task-hyper_eeg.fif\".format(subj_pair)\n",
    "    behav_path = op.join(behav_dir, \"{0}.csv\".format(subj_pair))\n",
    "\n",
    "    combined_raw = mne.io.read_raw_fif(subs_path, preload=True)\n",
    "\n",
    "    # split the subjects and delete the raw file\n",
    "    raws = split_raws(combined_raw)\n",
    "    del combined_raw\n",
    "    \n",
    "    for i, _ in enumerate(raws):\n",
    "        # set the EEG Montage. We use 64 chans from the standard 10-05 system.\n",
    "        montage = mne.channels.make_standard_montage(\"standard_1005\")\n",
    "        raws[i].set_montage(montage)\n",
    "\n",
    "    # combine the subjects again\n",
    "    raw_combined = combine_raws(raws[0], raws[1])\n",
    "    del raws  # to save memory\n",
    "    \n",
    "    # filter\n",
    "    raw_combined.filter(l_freq=0.1, h_freq=120)\n",
    "    \n",
    "    # define the epoching window\n",
    "    tmin = 0\n",
    "    tmax = 1.5\n",
    "\n",
    "    # do the behavioral analysis and get the epochs\n",
    "    behavioral_df = calculate_alpha(pd.read_csv(behav_path))\n",
    "    event_df = create_event_df(raw_combined)\n",
    "    event_df = remove_ghost_triggers(event_df)\n",
    "    event_df = join_event_dfs(event_df, behavioral_df)\n",
    "    \n",
    "    # get the first tap by looking at the first sample in each trial\n",
    "    min_idx = event_df.groupby([\"trial\"])[\"sample\"].idxmin()\n",
    "    early_df = event_df[event_df.index.isin(min_idx)]\n",
    "    early_events = events_from_event_df(early_df)\n",
    "    early_events[:,-1] = 1\n",
    "    \n",
    "    # get the late taps by looking at the last sample - 1.5 seconds\n",
    "    max_idx = event_df.groupby([\"trial\"])[\"sample\"].idxmax()\n",
    "    late_df = event_df[event_df.index.isin(max_idx)]\n",
    "    late_events = events_from_event_df(late_df)\n",
    "    late_events[:,0] -= int(raw_combined.info[\"sfreq\"] * 1.5)\n",
    "    late_events[:,-1] = 2\n",
    "    \n",
    "    # get the baseline events (an equally scaled window right before the early epochs)\n",
    "    base_events = early_events.copy()\n",
    "    base_events[:,0] -= int(raw_combined.info[\"sfreq\"] * (tmax - tmin))\n",
    "    base_events[:,-1] = 0\n",
    "\n",
    "    # define the parameters for epoching\n",
    "    combined_events = np.vstack([base_events, early_events, late_events])\n",
    "\n",
    "    # epoch the data. Here we filter out bad segments from both participants\n",
    "    epochs = mne.Epochs(raw_combined, combined_events, tmin=tmin, tmax=tmax,\n",
    "                        picks=[\"eeg\"], baseline=(0, 0), preload=True) # only use the first two epochs\n",
    "    epochs.event_id = dict(baseline=0, early=1, late=2)\n",
    "    \n",
    "    \n",
    "    # we have to combine both autoreject thresholds first and remove the manually\n",
    "    #rejects = [load_autoreject(\"sub-{0}_p-{1}\".format(subj_pair, i)).get_reject_log(split_epochs(epochs)[i]).bad_epochs for i in range(2)]\n",
    "    #combined_rejects = np.logical_or(rejects[0], rejects[1])\n",
    "    \n",
    "    event_types = [\"baseline\", \"early\", \"late\"]\n",
    "    \n",
    "\n",
    "    rejects = []\n",
    "    for i in range(2):\n",
    "        cond_rejects = []\n",
    "        for condition in event_types:\n",
    "            subj_id = \"sub-{0}_p-{1}-{2}\".format(subj_pair, i, condition)\n",
    "            cur_reject = load_autoreject(subj_id).get_reject_log(split_epochs(epochs)[i][condition]).bad_epochs\n",
    "            cond_rejects.append(cur_reject)\n",
    "        \n",
    "        rejects.append(np.logical_or(np.logical_or(cond_rejects[0], cond_rejects[1]), cond_rejects[2]))\n",
    "    combined_rejects = np.hstack([np.logical_or(rejects[0], rejects[1]) for i in range(3)])\n",
    "\n",
    "    \n",
    "    #rejects = np.hstack([[load_autoreject(\"sub-{0}_p-{1}-{2}\".format(subj_pair, i, condition)).get_reject_log(split_epochs(epochs[condition])[i]).bad_epochs for i in range(2)] for condition in event_types])\n",
    "    #combined_rejects = np.logical_or(rejects[0], rejects[1])\n",
    "    \n",
    "    # apply the heuristic to reject all parts of a trial if 2 or more epochs out of\n",
    "    # baseline, early, and late, are bad.\n",
    "    #bad_trials = np.vstack([combined_rejects[:300],\n",
    "    #                        combined_rejects[300:600],\n",
    "    #                        combined_rejects[600:]])\n",
    "    #bad_trial_sets = np.sum(bad_trials, axis=0) >= 1\n",
    "    #combined_rejects = np.hstack([bad_trial_sets] * 3)\n",
    "    \n",
    "    epochs = epochs.drop(combined_rejects, reason=\"Autoreject\")\n",
    "    \n",
    "    # save the dropped epochs\n",
    "    drop_list = [ind for ind, val in enumerate(epochs.drop_log[:300]) if val == [\"Autoreject\"]]\n",
    "    drop_fname = op.join(result_dir, \"dropped_epochs\", subj_pair + \".mat\")\n",
    "    savemat(drop_fname, {\"drop_list\":drop_list})\n",
    "\n",
    "    \n",
    "     # split the epochs to apply ICA and TFR transform\n",
    "    epochs_split = list(split_epochs(epochs))\n",
    "    \n",
    "    # frequencies should be 25 freqs, log spaced between 4 and 50\n",
    "    freqs = np.logspace(np.log10(4), np.log10(45), 20)#[:2] # TODO: only here for debugging! remove again!\n",
    "    cycles = freqs / 2.\n",
    "    \n",
    "    for i, cur_eps in enumerate(epochs_split):\n",
    "        subj_id = \"sub-{0}_p-{1}\".format(subj_pair, i)\n",
    "        \n",
    "        condition_split = []\n",
    "        for condition in event_types:\n",
    "        \n",
    "            cur_cond_eps = cur_eps[condition]\n",
    "            \n",
    "            # apply autoreject (exclude bads and interpolate)\n",
    "            # TODO: The Autoreject guys apply ICA first and then autoreject local. i would do the same\n",
    "            # Applying ICA first will look ugly, but for the first pair, it saves ~ 50 epochs\n",
    "            ar = load_autoreject(subj_id + \"-\" + condition)\n",
    "            reject_log = ar.get_reject_log(cur_cond_eps)\n",
    "            #cur_cond_eps = ar.transform(cur_cond_eps, return_log=False)\n",
    "\n",
    "            # apply ICA\n",
    "            ica = load_ica(subj_id)\n",
    "            cur_cond_eps = ica.apply(cur_cond_eps)\n",
    "            \n",
    "            # interpolate channels from autoreject\n",
    "            _apply_interp(reject_log, cur_cond_eps, ar.threshes_,\n",
    "                          ar.picks_, ar.dots, ar.verbose)\n",
    "            \n",
    "            condition_split.append(cur_cond_eps)\n",
    "            \n",
    "        # concatenate the different conditions again\n",
    "        cur_eps = mne.concatenate_epochs(condition_split)\n",
    "        \n",
    "        # rereference to avg ref\n",
    "        cur_eps.set_eeg_reference(ref_channels='average')\n",
    "\n",
    "        # apply surface laplacian\n",
    "        cur_eps = mne.preprocessing.compute_current_source_density(cur_eps,\n",
    "                                                                   stiffness=4,\n",
    "                                                                   lambda2=1e-5)\n",
    "        \n",
    "        epochs_split[i] = cur_eps\n",
    "        \n",
    "        \n",
    "    #combine the epochs again\n",
    "    epochs = combine_epochs(epochs_split[0], epochs_split[1])\n",
    "    \n",
    "    \n",
    "                         \n",
    "    for condition in event_types:\n",
    "    \n",
    "        # Get the phase angles via a wavelet transform\n",
    "        phases = mne.time_frequency.tfr_morlet(epochs[condition], freqs, cycles, output=\"phase\",\n",
    "                                               return_itc=False, average=False, n_jobs=n_jobs)\n",
    "        \n",
    "        # save the condition\n",
    "        fname = op.join(result_dir, \"phase_angles\", subj_pair + \"_\" + condition)\n",
    "        phases.save(fname)\n",
    "        del phases\n",
    "        \n",
    "    # subtract the baseline\n",
    "    #phase_angles[\"early\"].data -= phase_angles[\"baseline\"].data\n",
    "    #phase_angles[\"late\"].data -= phase_angles[\"baseline\"].data\n",
    "    \n",
    "    # delete some stuff to free some memory\n",
    "    #del phase_angles[\"baseline\"]\n",
    "    del epochs\n",
    "    del epochs_split\n",
    "    del cur_eps\n",
    "    del condition_split\n",
    "    del cur_cond_eps\n",
    "    del raw_combined\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "174"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(drop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /net/store/nbp/projects/hyperscanning/study_project/results/phase_angles/202_early ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-134625a7d2dd>:8: RuntimeWarning: This filename (/net/store/nbp/projects/hyperscanning/study_project/results/phase_angles/202_early) does not conform to MNE naming conventions. All tfr files should end with -tfr.h5 or _tfr.h5\n",
      "  epochs = mne.time_frequency.read_tfrs(fname)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n"
     ]
    }
   ],
   "source": [
    "import os.path as op\n",
    "import mne\n",
    "result_dir = \"/net/store/nbp/projects/hyperscanning/study_project/results\"\n",
    "subj_pair = \"202\"\n",
    "condition = \"early\"\n",
    "fname = op.join(result_dir, \"phase_angles\", subj_pair + \"_\" + condition)\n",
    "\n",
    "epochs = mne.time_frequency.read_tfrs(fname)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b26ff9b14205>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mepochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'epochs' is not defined"
     ]
    }
   ],
   "source": [
    "epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    \n",
    "     # split the epochs to apply ICA and TFR transform\n",
    "    epochs_split = list(split_epochs(epochs))\n",
    "    \n",
    "    # frequencies should be 25 freqs, log spaced between 4 and 50\n",
    "    freqs = np.logspace(np.log10(4), np.log10(45), 20)#[:2] # TODO: only here for debugging! remove again!\n",
    "    cycles = freqs / 2.\n",
    "    \n",
    "    for i, cur_eps in enumerate(epochs_split):\n",
    "        subj_id = \"sub-{0}_p-{1}\".format(subj_pair, i)\n",
    "        \n",
    "        condition_split = []\n",
    "        for condition in event_types:\n",
    "        \n",
    "            cur_cond_eps = cur_eps[condition]\n",
    "            \n",
    "            # apply autoreject (exclude bads and interpolate)\n",
    "            # TODO: The Autoreject guys apply ICA first and then autoreject local. i would do the same\n",
    "            # Applying ICA first will look ugly, but for the first pair, it saves ~ 50 epochs\n",
    "            ar = load_autoreject(subj_id + \"-\" + condition)\n",
    "            reject_log = ar.get_reject_log(cur_cond_eps)\n",
    "            #cur_cond_eps = ar.transform(cur_cond_eps, return_log=False)\n",
    "\n",
    "            # apply ICA\n",
    "            ica = load_ica(subj_id)\n",
    "            cur_cond_eps = ica.apply(cur_cond_eps)\n",
    "            \n",
    "            # interpolate channels from autoreject\n",
    "            _apply_interp(reject_log, cur_cond_eps, ar.threshes_,\n",
    "                          ar.picks_, ar.dots, ar.verbose)\n",
    "            \n",
    "            condition_split.append(cur_cond_eps)\n",
    "            \n",
    "        # concatenate the different conditions again\n",
    "        cur_eps = mne.concatenate_epochs(condition_split)\n",
    "        \n",
    "        # rereference to avg ref\n",
    "        cur_eps.set_eeg_reference(ref_channels='average')\n",
    "\n",
    "        # apply surface laplacian\n",
    "        cur_eps = mne.preprocessing.compute_current_source_density(cur_eps,\n",
    "                                                                   stiffness=4,\n",
    "                                                                   lambda2=1e-5)\n",
    "        \n",
    "        epochs_split[i] = cur_eps\n",
    "        \n",
    "        \n",
    "    #combine the epochs again\n",
    "    epochs = combine_epochs(epochs_split[0], epochs_split[1])\n",
    "    \n",
    "    \n",
    "    # initialize containers to analyze later\n",
    "    phase_angles = {}\n",
    "    ispc_matrices = {}\n",
    "    small_worlds = {}\n",
    "                         \n",
    "    for condition in event_types:\n",
    "    \n",
    "        # Get the phase angles via a wavelet transform\n",
    "        phases = mne.time_frequency.tfr_morlet(epochs[condition], freqs, cycles, output=\"phase\",\n",
    "                                               return_itc=False, average=False, n_jobs=n_jobs)\n",
    "        phase_angles[condition] = phases#[:8] # TODO: only here for debugging! remove again!\n",
    "        \n",
    "    # subtract the baseline\n",
    "    phase_angles[\"early\"].data -= phase_angles[\"baseline\"].data\n",
    "    phase_angles[\"late\"].data -= phase_angles[\"baseline\"].data\n",
    "    \n",
    "    # delete some stuff to free some memory\n",
    "    del phase_angles[\"baseline\"]\n",
    "    del epochs\n",
    "    del epochs_split\n",
    "    del cur_eps\n",
    "    del condition_split\n",
    "    del cur_cond_eps\n",
    "    del raw_combined\n",
    "    \n",
    "    # save the data\n",
    "    for condition in [\"early\", \"late\"]:\n",
    "        \n",
    "        fname = op.join(result_dir, \"phase_angles\", subj_pair + \"_\" + condition + \".hdf5\")\n",
    "        h5f = h5py.File(fname, \"w\")\n",
    "        h5f.create_dataset(condition, data=phase_angles[condition].data)\n",
    "        h5f.close()\n",
    "            \n",
    "        savemat(op.join(result_dir, \"phase_angles\", subj_pair + \"_times.mat\"),\n",
    "                {\"times\":phase_angles[condition].times})\n",
    "        del phase_angles[condition]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In /net/store/nbp/projects/hyperscanning/study_project/programming_tools/miniconda3/envs/hyperscanning/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /net/store/nbp/projects/hyperscanning/study_project/programming_tools/miniconda3/envs/hyperscanning/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /net/store/nbp/projects/hyperscanning/study_project/programming_tools/miniconda3/envs/hyperscanning/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In /net/store/nbp/projects/hyperscanning/study_project/programming_tools/miniconda3/envs/hyperscanning/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /net/store/nbp/projects/hyperscanning/study_project/programming_tools/miniconda3/envs/hyperscanning/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /net/store/nbp/projects/hyperscanning/study_project/programming_tools/miniconda3/envs/hyperscanning/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /net/store/nbp/projects/hyperscanning/study_project/programming_tools/miniconda3/envs/hyperscanning/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /net/store/nbp/projects/hyperscanning/study_project/programming_tools/miniconda3/envs/hyperscanning/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /net/store/nbp/projects/hyperscanning/study_project/results/phase_angles/203_baseline ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-89c502e6a74c>:36: RuntimeWarning: This filename (/net/store/nbp/projects/hyperscanning/study_project/results/phase_angles/203_baseline) does not conform to MNE naming conventions. All tfr files should end with -tfr.h5 or _tfr.h5\n",
      "  baseline = mne.time_frequency.read_tfrs(op.join(result_dir, \"phase_angles\", subj_pair + \"_baseline\"))[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "Calculating ISPCs for: 203, condition early\n",
      "Reading /net/store/nbp/projects/hyperscanning/study_project/results/phase_angles/203_early ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-89c502e6a74c>:42: RuntimeWarning: This filename (/net/store/nbp/projects/hyperscanning/study_project/results/phase_angles/203_early) does not conform to MNE naming conventions. All tfr files should end with -tfr.h5 or _tfr.h5\n",
      "  phase_angles = mne.time_frequency.read_tfrs(op.join(result_dir, \"phase_angles\", subj_pair + \"_\" + condition + \"\"))[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path as op\n",
    "module_path = op.abspath(op.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import numpy as np\n",
    "import mne\n",
    "import h5py\n",
    "from scipy.io import savemat, loadmat\n",
    "\n",
    "from data_analysis.functions_connectivity import \\\n",
    "    epochs_ispc, multi_ispc\n",
    "from data_analysis.functions_graph_theory import \\\n",
    "    multi_small_world\n",
    "\n",
    "# define which ISPCS to calculate\n",
    "#['202','203','204','205','206','207','208','209','211','212']\n",
    "subj_pairs = ['203']\n",
    "\n",
    "# conditions [\"early\", \"late\"]\n",
    "conditions = [\"early\"]\n",
    "\n",
    "# number of cores to use for parallel processing (ramsauer pc should have 80 cores)\n",
    "n_jobs = 31\n",
    "\n",
    "subject_dir = \"/net/store/nbp/projects/hyperscanning/hyperscanning-2.0/mne_data/sourcedata/\"\n",
    "behav_dir = \"/net/store/nbp/projects/hyperscanning/study_project/NBP_Hyperscanning/data_analysis/Behavioural_Analysis/BehaviouralData\"\n",
    "result_dir = \"/net/store/nbp/projects/hyperscanning/study_project/results\"\n",
    "\n",
    "\n",
    "# calculate the ISPC ###################################\n",
    "for subj_pair in subj_pairs:\n",
    "    \n",
    "    # get the baseline\n",
    "    baseline = mne.time_frequency.read_tfrs(op.join(result_dir, \"phase_angles\", subj_pair + \"_baseline\"))[0]\n",
    "    \n",
    "    for condition in conditions:\n",
    "        print(\"Calculating ISPCs for: {0}, condition {1}\".format(subj_pair, condition), flush=True)\n",
    "        \n",
    "        # load the phase_angles from data\n",
    "        phase_angles = mne.time_frequency.read_tfrs(op.join(result_dir, \"phase_angles\", subj_pair + \"_\" + condition + \"\"))[0]\n",
    "        \n",
    "        # subtract the baseline\n",
    "        phase_angles.data -= baseline.data\n",
    "        \n",
    "        # ISPC\n",
    "        ispc_matrices = multi_ispc(phase_angles, n_jobs=n_jobs)\n",
    "        \n",
    "        # save the first batch of data\n",
    "        savemat(op.join(result_dir, \"ispc_matrices\", subj_pair + \"_\" + condition + \".mat\"),\n",
    "                {condition:ispc_matrices})\n",
    "        del phase_angles\n",
    "        del ispc_matrices\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
