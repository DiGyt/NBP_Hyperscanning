{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and define subject path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/net/store/nbp/projects/hyperscanning/study_project/programming_tools/miniconda3/envs/hyperscanning/lib/python3.8/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'module://ipympl.backend_nbagg'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is our preprocessing script.\n",
    "# We select one recording, clean stuff and then perform the ICA\n",
    "# During this process, we save all bad components segments and channels\n",
    "\n",
    "import sys\n",
    "import os.path as op\n",
    "module_path = op.abspath(op.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mne\n",
    "from autoreject import AutoReject, get_rejection_threshold\n",
    "\n",
    "from data_analysis.functions_preprocessing import \\\n",
    "    (split_raws, mark_bads, save_bads, run_ica, save_ica,\n",
    "     save_autoreject, load_autoreject, BAD_AR_PATH)\n",
    "from data_analysis.functions_behavioral import \\\n",
    "    (create_event_df, remove_ghost_triggers, calculate_alpha,\n",
    "     join_event_dfs, remove_outliers, events_from_event_df)\n",
    "\n",
    "subject_dir = '/net/store/nbp/projects/hyperscanning/hyperscanning-2.0/mne_data/sourcedata/'\n",
    "behav_dir = \"/net/store/nbp/projects/hyperscanning/study_project/NBP_Hyperscanning/data_analysis/Behavioural_Analysis/BehaviouralData\"\n",
    "\n",
    "\n",
    "%matplotlib widget\n",
    "import matplotlib\n",
    "matplotlib.get_backend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the subject you want to clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please Type in, which subject pair you want to clean.\n",
      "For the pilot study, possible choices are:\n",
      "[202, 203, 204, 205, 206, 207, 208, 209, 211, 212]\n",
      "202\n",
      "\n",
      "Please Type in, which subject pair you want to clean.\n",
      "Type: 0 for the first participant and: 1 for the second.\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "subj_pair = input(\"Please Type in, which subject pair you want to clean.\\n\"\n",
    "                  \"For the pilot study, possible choices are:\\n\"\n",
    "                  \"[202, 203, 204, 205, 206, 207, 208, 209, 211, 212]\\n\")\n",
    "\n",
    "participant = input(\"\\nPlease Type in, which subject pair you want to clean.\\n\"\n",
    "                    \"Type: 0 for the first participant and: 1 for the second.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare the EEG recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening raw data file /net/store/nbp/projects/hyperscanning/hyperscanning-2.0/mne_data/sourcedata/sub-202/eeg/sub-202_task-hyper_eeg.fif...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-e8facb888f33>:11: RuntimeWarning: This filename (/net/store/nbp/projects/hyperscanning/hyperscanning-2.0/mne_data/sourcedata/sub-202/eeg/sub-202_task-hyper_eeg.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz or _meg.fif\n",
      "  combined_raw = mne.io.read_raw_fif(subs_path, preload=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Isotrak not found\n",
      "    Range : 0 ... 3725311 =      0.000 ...  3637.999 secs\n",
      "Ready.\n",
      "Opening raw data file /net/store/nbp/projects/hyperscanning/hyperscanning-2.0/mne_data/sourcedata/sub-202/eeg/sub-202_task-hyper_eeg-1.fif...\n",
      "Isotrak not found\n",
      "    Range : 3725312 ... 5675445 =   3638.000 ...  5542.427 secs\n",
      "Ready.\n",
      "Reading 0 ... 5675445  =      0.000 ...  5542.427 secs...\n",
      "EEG channel type selected for re-referencing\n",
      "Applying a custom EEG reference.\n",
      "EEG channel type selected for re-referencing\n",
      "Applying a custom EEG reference.\n",
      "EEG channel type selected for re-referencing\n",
      "Applying a custom EEG reference.\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.1 - 1.2e+02 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.10\n",
      "- Lower transition bandwidth: 0.10 Hz (-6 dB cutoff frequency: 0.05 Hz)\n",
      "- Upper passband edge: 120.00 Hz\n",
      "- Upper transition bandwidth: 30.00 Hz (-6 dB cutoff frequency: 135.00 Hz)\n",
      "- Filter length: 33793 samples (33.001 sec)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Raw | sub-202_task-hyper_eeg.fif, 80 x 5675446 (5542.4 s), ~3.38 GB, data loaded>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the subjects id and its path\n",
    "subj_id = \"sub-{0}_p-{1}\".format(subj_pair, participant)\n",
    "subs_path = subject_dir + \"sub-{0}/eeg/sub-{0}_task-hyper_eeg.fif\".format(subj_pair)\n",
    "behav_path = op.join(behav_dir, \"{0}.csv\".format(subj_pair))\n",
    "\n",
    "## overwrite it for the test\n",
    "# TODO: This line should be removed for the actual cleaning\n",
    "#subj_id = \"test_2\"\n",
    "\n",
    "# load the data\n",
    "combined_raw = mne.io.read_raw_fif(subs_path, preload=True)\n",
    "\n",
    "# split the subjects and delete the raw file\n",
    "raw = split_raws(combined_raw)[int(participant)]\n",
    "del combined_raw\n",
    "\n",
    "# set reference\n",
    "raw.set_eeg_reference([\"Cz\"])\n",
    "\n",
    "# set the EEG Montage. We use 64 chans from the standard 10-05 system.\n",
    "montage = mne.channels.make_standard_montage(\"standard_1005\")\n",
    "raw.set_montage(montage)\n",
    "\n",
    "# filter\n",
    "raw.filter(l_freq=0.1, h_freq=120)\n",
    "#raw.notch_filter(freqs=[50]) # notch filters were put out because they did not seem to add to the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define events and epoch the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6648 events found\n",
      "Event IDs: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 36 37 38 39 40 41 42 43 44 45 46 48 49]\n"
     ]
    }
   ],
   "source": [
    "# define the window length for for epoching\n",
    "tmin = 0\n",
    "tmax = 1.5\n",
    "\n",
    "# do the behavioral analysis and get the epochs\n",
    "behavioral_df = calculate_alpha(pd.read_csv(behav_path))\n",
    "event_df = create_event_df(raw)\n",
    "event_df = remove_ghost_triggers(event_df)\n",
    "event_df = join_event_dfs(event_df, behavioral_df)\n",
    "\n",
    "# get the first tap by looking at the first sample in each trial\n",
    "min_idx = event_df.groupby([\"trial\"])[\"sample\"].idxmin()\n",
    "early_df = event_df[event_df.index.isin(min_idx)]\n",
    "early_events = events_from_event_df(early_df)\n",
    "\n",
    "# get the late taps by looking at the last sample - 1.5 seconds\n",
    "max_idx = event_df.groupby([\"trial\"])[\"sample\"].idxmax()\n",
    "late_df = event_df[event_df.index.isin(max_idx)]\n",
    "late_events = events_from_event_df(late_df)\n",
    "late_events[:,0] -= int(raw.info[\"sfreq\"] * (tmax - tmin))\n",
    "\n",
    "# get the baseline events (an equally scaled window right before the early epochs)\n",
    "base_events = early_events.copy()\n",
    "base_events[:,0] -= int(raw.info[\"sfreq\"] * (tmax - tmin))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run autoreject on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some autoreject files for this subject already exist.Remove them from the BAD_AUTOREJECT PATH if you want to overwrite them.Else, existing ARs will be loaded.\n",
      "300 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "0 projection items activated\n",
      "Loading data for 300 events and 1537 original time points ...\n",
      "0 bad epochs dropped\n",
      "Loading preexisting AR: /net/store/nbp/projects/hyperscanning/study_project/marked_data/bad_autoreject/sub-202_p-0-baseline-ar.hdf5\n",
      "Estimating rejection dictionary for eeg\n",
      "Estimating rejection dictionary for eog\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c88ac9b8174f4dfa86c28d0a5505e582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a4a2198f7e740b6a673a462ff5a6703",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rejected 11 out of 300 epochs.\n",
      "300 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "0 projection items activated\n",
      "Loading data for 300 events and 1537 original time points ...\n",
      "0 bad epochs dropped\n",
      "Loading preexisting AR: /net/store/nbp/projects/hyperscanning/study_project/marked_data/bad_autoreject/sub-202_p-0-early-ar.hdf5\n",
      "Estimating rejection dictionary for eeg\n",
      "Estimating rejection dictionary for eog\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86c9ec5427d24f17b620c3ee614df700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8096a97373f472d938b164d15efbff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rejected 51 out of 300 epochs.\n",
      "300 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "0 projection items activated\n",
      "Loading data for 300 events and 1537 original time points ...\n",
      "0 bad epochs dropped\n",
      "Loading preexisting AR: /net/store/nbp/projects/hyperscanning/study_project/marked_data/bad_autoreject/sub-202_p-0-late-ar.hdf5\n",
      "Estimating rejection dictionary for eeg\n",
      "Estimating rejection dictionary for eog\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f21bcf8eb0ad45639c0fb0c869e54e99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b5f601b811143f697ca9d8b9375b036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rejected 1 out of 300 epochs.\n"
     ]
    }
   ],
   "source": [
    "# check if any of the autoreject files already exists\n",
    "ar_paths = [op.join(BAD_AR_PATH, subj_id + \"-\" + key + \"-ar.hdf5\") for key in [\"baseline\", \"early\", \"late\"]]\n",
    "if any(op.isfile(ar_path) for ar_path in ar_paths):\n",
    "    print(\"Some autoreject files for this subject already exist.\"\n",
    "          \"Remove them from the BAD_AUTOREJECT PATH if you want to overwrite them.\"\n",
    "          \"Else, existing ARs will be loaded.\")\n",
    "  \n",
    "\n",
    "epochs_list = []\n",
    "reject_list = []\n",
    "for index, (key, events) in enumerate({\"baseline\":base_events,\n",
    "                                       \"early\":early_events,\n",
    "                                       \"late\":late_events}.items()):\n",
    "\n",
    "    epochs = mne.Epochs(raw, events, tmin=tmin, tmax=tmax,\n",
    "                        baseline=(0, 0), preload=True)  #baseline=(0, 0)\n",
    "\n",
    "    picks = mne.pick_types(epochs.info, eeg=True)\n",
    "    \n",
    "    if op.isfile(ar_paths[index]):\n",
    "        # load an existing AR\n",
    "        print(\"Loading preexisting AR: \" + ar_paths[index])\n",
    "        ar = load_autoreject(subj_id + \"-\" + key)\n",
    "    else:\n",
    "        # define an autoreject object\n",
    "        ar = AutoReject(consensus=[0.1, 0.2, 0.3, 0.4, 0.5], thresh_method=\"random_search\",\n",
    "                        picks=picks, verbose=\"tqdm_notebook\") # [0.1, 0.2, 0.3, 0.4, 0.5], \"bayesian_optimization\"\n",
    "        # fit the epochs\n",
    "        ar.fit(epochs)\n",
    "        \n",
    "        # save the autoreject\n",
    "        save_autoreject(ar, subj_id + \"-\" + key)\n",
    "\n",
    "    # get the rejection threshold for ICA\n",
    "    reject = get_rejection_threshold(epochs)\n",
    "\n",
    "    # plot it\n",
    "    reject_log = ar.get_reject_log(epochs)\n",
    "    reject_log.plot()\n",
    "    \n",
    "    # plot the rejected epochs\n",
    "    scalings = dict(eeg=12e-5, eog=150e-6, misc=1e-3)\n",
    "    reject_log.plot_epochs(epochs, scalings=scalings)\n",
    "    print(\"Rejected {} out of {} epochs.\".format(sum(reject_log.bad_epochs), len(epochs)))\n",
    "    \n",
    "    # remove the bad epochs and add them to the epochs list\n",
    "    epochs_list.append(epochs[~reject_log.bad_epochs])\n",
    "    \n",
    "    # add the rejects to the reject list\n",
    "    reject_list.append(reject)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate the autoreject cleaned epochs and their average reject thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All reject dicts:  [{'eeg': 0.0007791586913267323, 'eog': 0.00013049609342052904}, {'eeg': 0.000409155837847659, 'eog': 0.0003538066397317241}, {'eeg': 0.00021534763122803052, 'eog': 0.00011624218720651669}]\n",
      "Average reject dict:  {'eeg': 0.00046788738680080726, 'eog': 0.00020018164011958994}\n",
      "837 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "Not setting metadata\n",
      "0 bad epochs dropped\n"
     ]
    }
   ],
   "source": [
    "def dict_key_mean(dict_list):\n",
    "    \"\"\"Calculate the mean value for each key between multiple dicts.\n",
    "    To return correct results, each key must be present in all of the\n",
    "    dicts in dict_list.\"\"\"\n",
    "    import collections, functools, operator \n",
    "    # sum the values with same keys \n",
    "    sum_dict = dict(functools.reduce(operator.add, \n",
    "                                     map(collections.Counter, dict_list)))\n",
    "    return {key: val/len(dict_list) for key, val in sum_dict.items()}\n",
    "\n",
    "\n",
    "reject = dict_key_mean(reject_list)\n",
    "print(\"All reject dicts: \", reject_list)\n",
    "print(\"Average reject dict: \", str(reject))\n",
    "\n",
    "epochs = mne.concatenate_epochs(epochs_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run (or load) the ICA and plot all components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up high-pass filter at 2 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal highpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 2.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 1.00 Hz)\n",
      "- Filter length: 1691 samples (1.651 sec)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-ba4c98de5995>:4: RuntimeWarning: filter_length (1691) is longer than the signal (1537), distortion is likely. Reduce filter length or filter a longer signal.\n",
      "  epochs.filter(l_freq=2, h_freq=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to ICA file already exists.\n",
      "Delete the respective ICA file (/net/store/nbp/projects/hyperscanning/study_project/marked_data/bad_components/sub-202_p-0-ica.fif)\n",
      "if you want to fit a new ICA.\n",
      " Loading existing ICA\n",
      "Reading /net/store/nbp/projects/hyperscanning/study_project/marked_data/bad_components/sub-202_p-0-ica.fif ...\n",
      "Now restoring ICA solution ...\n",
      "Ready.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78b630c9742940d9ac05d5c36f3df320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01058baca7784b15af1885a248d4929a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf82c5a7d8524890ae913bf5fa7ee23f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddbc58c8afd8486ba0e1c1076c0d8786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#print(len(epochs.copy().drop_bad(reject).pick_types(eeg=True).ch_names))\n",
    "\n",
    "# filter again for ICA\n",
    "epochs.filter(l_freq=2, h_freq=None)\n",
    "\n",
    "\n",
    "# run the ICA and save the marked components\n",
    "picks = list(mne.pick_types(epochs.info, eeg=True, exclude=[\"Cz\"]))\n",
    "ica = run_ica(epochs, subj_id, picks=picks, reject=reject,\n",
    "              n_components=63, method=\"fastica\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get ICA components based on their correlation with EOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using channel BIP1 as EOG channel\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3a4119c5506442dbda057ee92c0b704",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1197d9e7114f419bb92fa6ad3ff5e940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "837 matching events found\n",
      "No baseline correction applied\n",
      "Not setting metadata\n",
      "0 projection items activated\n",
      "0 bad epochs dropped\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d48e63a888c44f2b550e840fce13754",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "837 matching events found\n",
      "No baseline correction applied\n",
      "Not setting metadata\n",
      "0 projection items activated\n",
      "0 bad epochs dropped\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<Figure size 700x600 with 6 Axes>, <Figure size 700x600 with 6 Axes>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eog_name = \"BIP1\" if (participant == \"0\") else \"BIP5\"\n",
    "eog_idx, eog_scores = ica.find_bads_eog(epochs, ch_name=eog_name)\n",
    "\n",
    "# barplot of ICA component \"EOG match\" scores\n",
    "ica.plot_scores(eog_scores)\n",
    "\n",
    "# plot diagnostics\n",
    "ica.plot_properties(epochs, picks=eog_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose specific component properties to inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please type in which components you want to further inspect.\n",
      "E.G. 3, 4,15 for components 3, 4, and 15.\n",
      "0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a235566366434751b5f025bb1413c682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "837 matching events found\n",
      "No baseline correction applied\n",
      "Not setting metadata\n",
      "0 projection items activated\n",
      "0 bad epochs dropped\n"
     ]
    }
   ],
   "source": [
    "inp = input(\"Please type in which components you want to further \"\n",
    "            \"inspect.\\nE.G. 3, 4,15 for components 3, 4, and 15.\\n\")\n",
    "\n",
    "inp = [int(n) for n in inp.split(\",\") if n != \"\"]\n",
    "\n",
    "if len(inp) > 0:\n",
    "    ica.plot_properties(epochs, picks=inp, reject=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exclude specific ICA components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excluded ICA components:  [0]\n",
      "\n",
      "Please type in which components you want to exclude.\n",
      "E.G. 2, 3,14 for components 2, 3, and 14.\n",
      "1\n",
      "\n",
      "Excluded ICA components:  [0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Excluded ICA components: \", ica.exclude)\n",
    "\n",
    "inp = input(\"\\nPlease type in which components you want to exclude.\\n\"\n",
    "            \"E.G. 2, 3,14 for components 2, 3, and 14.\\n\")\n",
    "\n",
    "bad_comps = [int(comp) for comp in inp.split(\",\") if comp != \"\"]\n",
    "bad_comps = [comp for comp in set(bad_comps) if comp not in ica.exclude]\n",
    "\n",
    "ica.exclude.extend(bad_comps)\n",
    "print(\"\\nExcluded ICA components: \", ica.exclude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the ICA and its excluded components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you really want to save the components?\n",
      "Enter 'save' or 's' to save the data. Else, changes will be discarded.\n",
      "s\n",
      "Writing ICA solution to /net/store/nbp/projects/hyperscanning/study_project/marked_data/bad_components/sub-202_p-0-ica.fif...\n"
     ]
    }
   ],
   "source": [
    "inp = input(\"Do you really want to save the components?\\n\"\n",
    "            \"Enter 'save' or 's' to save the data. Else, \"\n",
    "            \"changes will be discarded.\\n\")\n",
    "\n",
    "if inp[0] == \"s\":\n",
    "    save_ica(ica, subj_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### give everyone access to the new marked files you've created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /net/store/nbp/projects/hyperscanning/study_project\n",
    "!chown -hR $USER:nbp *; chmod -R 770 *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Everything done. Thanks for cleaning :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
